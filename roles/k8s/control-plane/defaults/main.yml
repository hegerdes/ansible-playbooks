---
k8s_version: 1.34.2
k8s_dual_stack: false
k8s_cp_ping_check: true
k8s_install_core_dns: true
k8s_install_kube_proxy: true
k8s_native_ipv6_with_distinct_ip_pools: false
k8s_image_repo: registry.k8s.io
k8s_cluster_name: "{{ inventory_dir | basename }}"
k8s_pki_cert_path: /etc/kubernetes/pki
k8s_kubeconfig: "/etc/kubernetes/admin.conf"
k8s_shared_api_server_endpoint: "{{ ansible_facts.default_ipv4.address }}"
k8s_init_verbose: "{{ lookup('env', 'CI') | default('false', true) | bool == false }}"

# Service & Pod CIDR
k8s_pod_network_cidr: >-
  {{
  [k8s_pod_network_cidr_ipv4,k8s_pod_network_cidr_ipv6]
  | join(",")
  if(k8s_dual_stack) else
  k8s_pod_network_cidr_ipv4
  }}
k8s_service_network_cidr: >-
  {{
  [k8s_service_network_cidr_ipv4,k8s_service_network_cidr_ipv6]
  | join(",")
  if(k8s_dual_stack) else
  k8s_service_network_cidr_ipv4
  }}

k8s_pod_network_cidr_ipv4: 10.244.0.0/16
k8s_service_network_cidr_ipv4: 10.96.0.0/16
k8s_pod_network_cidr_ipv6: fd00:10:244::/48
k8s_service_network_cidr_ipv6: fd00:20:123::/112
k8s_node_pod_cidr: >-
  {{
  k8s_node_ip
  | ansible.utils.ipmath(2 | pow(56))
  | ansible.utils.ipmath(-1)
  if(k8s_native_ipv6_with_distinct_ip_pools) else
  "0.0.0.0"
  }}/{{ k8s_node_cidr_mask_size_ipv6 }}

k8s_clusterDNS: [10.96.0.10] # change for ipv6 only
k8s_node_cidr_mask_size_ipv4: 24
k8s_node_cidr_mask_size_ipv6: 64

# Kubeadm & Kubelet conf
k8s_kubelet_extra_conf: {}
k8s_kubelet_conf: '{{ lookup("template", "kubelet-conf.yml.j2") }}'
k8s_kubelet_override_conf: '{{ lookup("template", "kubelet-override.yml.j2") }}'
k8s_kubeadm_conf: '{{ lookup("template", "kubeadm-config.yml.j2") }}'
k8s_kubeadm_join_conf: '{{ lookup("template", "kubeadm-join-config.yml.j2") }}'
k8s_etcd_encryption_secret: my-super-secure-k8s-etcd-secret!
k8s_etcd_encryption_conf: '{{ lookup("template", "encryption-config.yml.j2") }}'
k8s_admission_config: '{{ lookup("template", "admission-control-config.yml.j2") }}'
k8s_admission_extra_plugins: [NodeRestriction, DenyServiceExternalIPs]
k8s_admission_plugin_config:
  - '{{ lookup("file", "pod-security-admission.yaml") | from_yaml }}'

# IPs & cert sans
k8s_node_ip: "{{ k8s_node_ip_v4 }}"
k8s_node_ip_v4: "{{ ansible_facts.default_ipv4.address }}"
k8s_node_ip_v6: "{{ ansible_facts.default_ipv6.address }}"
k8s_resolve_conf: /etc/resolv.conf
k8s_advertise_address: '{{ k8s_node_ip | split(",") | first }}'
k8s_cert_extra_sans_ipv6: '{{ ["::1"] + ansible_facts.all_ipv6_addresses if(k8s_dual_stack) else [] }}'
k8s_cert_extra_sans: '{{ ["127.0.0.1", "localhost"] + ansible_facts.all_ipv4_addresses + k8s_cert_extra_sans_ipv6 }}'
k8s_install_k9s: true
k8s_save_admin_kubeconf_to_azure: false

# K8s cert key for cp join. Default one is generated
k8_cert_key: none

k8s_feature_gates: {}
# # Example
# k8s_feature_gates:
#   UserNamespacesSupport: true

k8s_feature_gates_args_string: >-
  {%- for k, v in k8s_feature_gates.items() -%}
  {{ k }}={{ v | lower }}{% if not loop.last %},{% endif %}
  {%- endfor -%}

# Docs https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init-phase/
k8s_kubeadm_skip_phases: []
# # Example
# k8s_kubeadm_skip_phases:
#   - addon/kube-proxy

# Docs: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/
k8s_controlplane_extra_args: []
# # Example
# k8s_controlplane_extra_args:
#   - name: anonymous-auth
#     value: "false"

# Docs: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/
k8s_controller_extra_args:
  - name: allocate-node-cidrs
    value: "{{ 'false' if k8s_native_ipv6_with_distinct_ip_pools else 'true' }}"
  - name: controllers
    value: "{{ k8s_controlplane_kube_controllers }}"

# Docs: https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/
k8s_scheduler_extra_args: []
# # Example
# k8s_controller_extra_args:
#   - name: log-flush-frequency
#     value: "5s"

k8s_controlplane_extra_envs: []
# # Example
# k8s_controlplane_extra_envs:
#   - name: hello
#     value: world

k8s_controlplane_extra_files: []
# # Example
# k8s_controlplane_extra_files:
#   - name: api-auth-conf
#     dst: /etc/kubernetes/k8s-api-auth-conf.yaml
#     content: "hello: world"
#     validate: yq . %s

# Apply kube patches
k8s_patches:
  - name: Allow CP to be LoadBalancer target
    api_version: v1
    when: >-
      {{ k8s_remove_no_schedule_cp_taints
      and 'node.kubernetes.io/exclude-from-external-load-balancers'
      in (__k8s_cp_node.resources | first).metadata.labels }}
    kind: Node
    resource_name: "{{ inventory_hostname }}"
    patches:
      - op: remove
        path: /metadata/labels/node.kubernetes.io~1exclude-from-external-load-balancers
  - name: Set CoreDNS Conf
    api_version: v1
    kind: ConfigMap
    resource_name: coredns
    namespace: kube-system
    patches:
      - op: replace
        path: /data/Corefile
        value: "{{ k8s_coredns_conf }}"
  - name: Set nodes podCIDR
    api_version: v1
    when: >-
      {{ k8s_pod_network_cidr is ansible.utils.ipv6
      and k8s_node_ip | ansible.utils.ipaddr('public')
      and k8s_native_ipv6_with_distinct_ip_pools
      }}
    kind: Node
    resource_name: "{{ inventory_hostname }}"
    patches:
      - op: add
        path: /spec/podCIDR
        value: "{{ k8s_node_pod_cidr }}"
      - op: add
        path: /spec/podCIDRs
        value:
          - "{{ k8s_node_pod_cidr }}"

######### Taints & Roles #########
k8s_remove_no_schedule_cp_taints: false
k8s_taints: []
# # Example
# k8s_taints:
# See https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#taint-v1-core
#   - key: node-role.kubernetes.io/control-plane
#     effect: NoSchedule

######### INIT-Manifests #########
k8s_init_resources_manifests: []
# # Example
# k8s_init_resources_manifests:
#   - name: oauth2-proxy
#     definition: '{{ lookup("template", k8s_init_resources.file) | from_yaml_all }}'
#     namespace: default      # optional
#     state: present          # optional
#     wait: true              # optional

# Trace conf - not used by default
k8s_controlplane_trace_conf:
  apiVersion: apiserver.config.k8s.io/v1beta1
  kind: TracingConfiguration
  endpoint: localhost:4317
  samplingRatePerMillion: 100

k8s_controlplane_kube_controllers: "{{ k8s_controlplane_all_kube_controllers | difference(k8s_controlplane_disabled_kube_controllers) | sort | join(',') }}"

k8s_controlplane_all_kube_controllers:
  - bootstrap-signer-controller
  - certificatesigningrequest-approving-controller
  - certificatesigningrequest-cleaner-controller
  - certificatesigningrequest-signing-controller
  - cloud-node-lifecycle-controller
  - clusterrole-aggregation-controller
  - cronjob-controller
  - daemonset-controller
  - deployment-controller
  - device-taint-eviction-controller
  - disruption-controller
  - endpoints-controller
  - endpointslice-controller
  - endpointslice-mirroring-controller
  - ephemeral-volume-controller
  - garbage-collector-controller
  - horizontal-pod-autoscaler-controller
  - job-controller
  - kube-apiserver-serving-clustertrustbundle-publisher-controller
  - legacy-serviceaccount-token-cleaner-controller
  - namespace-controller
  - node-ipam-controller
  - node-lifecycle-controller
  - node-route-controller
  - persistentvolume-attach-detach-controller
  - persistentvolume-binder-controller
  - persistentvolume-expander-controller
  - persistentvolume-protection-controller
  - persistentvolumeclaim-protection-controller
  - pod-garbage-collector-controller
  - podcertificaterequest-cleaner-controller
  - replicaset-controller
  - replicationcontroller-controller
  - resourceclaim-controller
  - resourcequota-controller
  - root-ca-certificate-publisher-controller
  - selinux-warning-controller
  - service-cidr-controller
  - service-lb-controller
  - serviceaccount-controller
  - serviceaccount-token-controller
  - statefulset-controller
  - storage-version-migrator-controller
  - storageversion-garbage-collector-controller
  - taint-eviction-controller
  - token-cleaner-controller
  - ttl-after-finished-controller
  - ttl-controller
  - validatingadmissionpolicy-status-controller
  - volumeattributesclass-protection-controller

k8s_controlplane_disabled_kube_controllers:
  - selinux-warning-controller
  - token-cleaner-controller
  - legacy-serviceaccount-token-cleaner-controller
  - replicationcontroller-controller
  - token-cleaner-controller
  - "{{ 'node-ipam-controller' if k8s_native_ipv6_with_distinct_ip_pools }}"

# Kubernetes structured authentication conf.
# Docs: https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens
k8s_controlplane_auth_n_conf: '{{ lookup("file", "authentication-conf.yaml") | from_yaml }}'
# Kubernetes structured authorization conf.
# Docs: https://kubernetes.io/docs/reference/access-authn-authz/authorization/
k8s_controlplane_auth_z_conf: '{{ lookup("file", "authorization-conf.yaml") | from_yaml }}'

k8s_init_helm_apps: []
# k8s_init_helm_apps:
#   - chart_name: demo-app
#     repo_name: gh-demo-app
#     repo_url: https://user.github.io/demo-app/
#     deploy_name: demo-app
#     # deploy_name: null
#     # deploy_namespace: null
#     chart_version: null
#     # repo_user: null
#     # repo_pw: null
#     chart_values:
#       my_key: my_val

k8s_default_manifests:
  - name: node-labels
    run_once: false
    definition:
      apiVersion: v1
      kind: Node
      metadata:
        name: "{{ inventory_hostname }}"
        labels:
          node-role.kubernetes.io/control-plane: "control-plane"
          node-os.kubernetes.io/arch: "{{ ansible_facts.architecture }}"
          node-os.kubernetes.io/os: "{{ ansible_facts.distribution }}"
          node-os.kubernetes.io/os-version: "{{ ansible_facts.distribution_version }}"
          node.kubernetes.io/image-id: "{{ hcloud_image_id | default('unknown') | quote }}"
          node.kubernetes.io/provider-id: "{{ hcloud_id | default('unknown') | quote }}"
  - name: runtime-manifests
    definition: "{{ k8s_runtime_classes_all | flatten }}"

k8s_runtime_classes_all:
  - "{{ [k8s_runc_manifest.definition] }}"
  - "{{ [k8s_crun_manifest.definition] }}"
  - "{{ [k8s_youki_manifest.definition] if(k8s_install_youki | default(true)) else [] }}"
  - "{{ [k8s_wasmedge_manifest.definition] if(k8s_install_wasi | default(true)) else [] }}"
  - "{{ [k8s_gVisor_manifest.definition] if(k8s_install_gvisor | default(true)) else [] }}"
  - "{{ [k8s_spin_manifest.definition] if(k8s_install_spin | default(true)) else [] }}"
  - "{{ [k8s_kata_manifest.definition] if(k8s_install_kata_containers | default(false)) else [] }}"

k8s_gVisor_manifest:
  name: gVisor RuntimeClass
  definition: "{{ lookup('template', role_path + '/files/k8s-runtime-class-gvisor.yaml') | from_yaml_all }}"

k8s_crun_manifest:
  name: crun RuntimeClass
  definition: "{{ lookup('template', role_path + '/files/k8s-runtime-class-crun.yaml') | from_yaml_all }}"

k8s_runc_manifest:
  name: runc RuntimeClass
  definition: "{{ lookup('template', role_path + '/files/k8s-runtime-class-runc.yaml') | from_yaml_all }}"

k8s_youki_manifest:
  name: youki RuntimeClass
  definition: "{{ lookup('template', role_path + '/files/k8s-runtime-class-youki.yaml') | from_yaml_all }}"

k8s_kata_manifest:
  name: kata RuntimeClass
  definition: "{{ lookup('template', role_path + '/files/k8s-runtime-class-kata.yaml') | from_yaml_all }}"

k8s_wasmedge_manifest:
  name: wasi RuntimeClass
  definition: "{{ lookup('template', role_path + '/files/k8s-runtime-class-wasi.yaml') | from_yaml_all }}"

k8s_spin_manifest:
  name: spin RuntimeClass
  definition: "{{ lookup('template', role_path + '/files/k8s-runtime-class-spin.yaml') | from_yaml_all }}"

######### CoreDNS #########
k8s_coredns_upstreams: tls://1.1.1.1 tls://[2606:4700:4700::1111]:853 tls://1.0.0.1 tls://[2606:4700:4700::1001]:853
k8s_coredns_conf: |-
  .:53 {
      errors
      health {
          lameduck 10s
      }
      ready
      log . {
          class error
      }
      kubernetes cluster.local in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
          ttl 30
      }
      prometheus :9153
      forward . {{ k8s_coredns_upstreams }} {
          health_check 5s
          max_concurrent 1000
      }
      cache {
          disable success cluster.local
          disable denial cluster.local
      }
      loop
      reload
      loadbalance
  }

######### CNI #########
# Which CNI flannel, calico or cilium
k8s_cni: cilium

# Calico helm chart setup
k8s_calico_ip_pools_ipv4:
  - cidr: '{{ k8s_pod_network_cidr if("," not in k8s_pod_network_cidr) else k8s_pod_network_cidr | split(",") | first }}'
    # natOutgoing: Enabled
    # encapsulation: IPIP
    # nodeSelector: all()
  # - blockSize: 26
  #   cidr: 10.48.0.0/21
  #   encapsulation: IPIP
  #   natOutgoing: Enabled
  #   nodeSelector: all()

k8s_calico_ip_pools_ipv6:
  - blockSize: 122
    cidr: '{{ k8s_pod_network_cidr if("," not in k8s_pod_network_cidr) else k8s_pod_network_cidr | split(",") | last }}'
    encapsulation: None
    natOutgoing: Enabled
    nodeSelector: all()

k8s_calico_chart_version: "{{ null | default(omit) }}"
k8s_calico_helm_values:
  # See https://docs.tigera.io/calico/3.26/getting-started/kubernetes/helm
  # See https://docs.tigera.io/calico/latest/networking/ipam/ipv6#enable-dual-stack
  installation:
    cni:
      type: Calico
    calicoNetwork:
      ipPools: '{{ k8s_calico_ip_pools_ipv4 + k8s_calico_ip_pools_ipv6 if("," in k8s_pod_network_cidr) else k8s_calico_ip_pools_ipv4 }}'

# Cilium helm chart setup
k8s_cilium_chart_version: "{{ null | default(omit) }}"
# See example setup: https://docs.cilium.io/en/stable/helm-reference/
# Kube-proxy-free: https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/
k8s_cilium_helm_values:
  cluster:
    name: "{{ k8s_cluster_name }}"

  k8sServiceHost: "{{ k8s_shared_api_server_endpoint }}"
  k8sServicePort: 6443
  kubeProxyReplacement: true
  rollOutCiliumPods: true
  annotateK8sNode: true

  l7Proxy: true
  gatewayAPI:
    enabled: true
    enableProxyProtocol: true
    externalTrafficPolicy: Local
  envoy:
    enabled: true
    rollOutPods: true
    # debug:
    #   admin:
    #     enabled: true
    #     port: 9901
    prometheus:
      serviceMonitor:
        enabled: true
  # MTU: 1450 # for Hetzner
  operator:
    rollOutPods: true
    replicas: 1

  endpointRoutes:
    enabled: true

  loadBalancer:
    algorithm: maglev
    serviceTopology: true
    # acceleration: best-effort # disabled, best-effort or native
    # mode: hybrid

  hubble:
    enabled: true
    rollOutPods: true
    relay:
      enabled: true
      rollOutPods: true
    ui:
      enabled: false
      rollOutPods: true
      ingress:
        enabled: true
        className: nginx
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt-prod-nginx
          nginx.ingress.kubernetes.io/auth-url: http://oauth2-proxy.oauth-proxy.svc.cluster.local/oauth2/auth
          nginx.ingress.kubernetes.io/auth-signin: https://oauth.k8s.henrikgerdes.me/oauth2/start?rd=https://$host$request_uri
        hosts:
          - hubble.k8s.henrikgerdes.me
        tls:
          - secretName: hubble-ing-tls
            hosts:
              - hubble.k8s.henrikgerdes.me
  # Needed if tailscale services are routed
  socketLB:
    hostNamespaceOnly: true

  # hostFirewall:
  #   enabled: true
  # # BGP native routing
  # bgp:
  #   enabled: true
  #   announce:
  #     loadbalancerIP: true
  # encryption:
  #   enabled: true
  #   type: wireguard

  ipam:
    mode: kubernetes
    operator:
      clusterPoolIPv4PodCIDRList: ["{{ k8s_pod_network_cidr_ipv4 }}"]
      # clusterPoolIPv6PodCIDRList: ["{{ k8s_pod_network_cidr_ipv6 }}"]

  # # For IPv6 set
  # autoDirectNodeRoutes: true
  # enableIPv6Masquerade: false
  # routingMode: native
  # ipv4:
  #   enabled: false
  # ipv6:
  #   enabled: true
  # bpf:
  #   masquerade: true # needs tp be off when using hostnat

system_arch_map:
  x86_64: amd64
  aarch64: arm64
